{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shieldstring/90DaysOfDevOps/blob/main/CIFAR_10_Multi_Class_Image_Classification_with_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. Load and Preprocess CIFAR-10 Dataset ---\n",
        "print(\"--- Loading and Preprocessing CIFAR-10 Dataset ---\")\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "# CIFAR-10 has 10 classes\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(\"Dataset loaded and preprocessed successfully.\")\n",
        "\n",
        "# Define class names for better readability in plots\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# --- 2. Define a function to create CNN models ---\n",
        "def create_cnn_model(input_shape, num_classes, architecture_type='simple_cnn', learning_rate=0.001, optimizer_type='adam'):\n",
        "    \"\"\"\n",
        "    Creates a Convolutional Neural Network (CNN) model with specified architecture and optimizer.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Shape of the input images (e.g., (32, 32, 3)).\n",
        "        num_classes (int): Number of output classes.\n",
        "        architecture_type (str): Defines the CNN architecture ('simple_cnn', 'deeper_cnn').\n",
        "        learning_rate (float): Learning rate for the optimizer.\n",
        "        optimizer_type (str): Type of optimizer ('adam', 'sgd').\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled Keras CNN model.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    if architecture_type == 'simple_cnn':\n",
        "        # Simple CNN architecture\n",
        "        model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.25)) # Regularization to prevent overfitting\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        model.add(Flatten()) # Flatten the 3D output to 1D for dense layers\n",
        "        model.add(Dense(512, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(num_classes, activation='softmax')) # Output layer with softmax for multi-class\n",
        "\n",
        "    elif architecture_type == 'deeper_cnn':\n",
        "        # Deeper CNN architecture with BatchNormalization\n",
        "        model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
        "        model.add(BatchNormalization()) # Helps stabilize and accelerate training\n",
        "        model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid architecture_type. Choose 'simple_cnn' or 'deeper_cnn'.\")\n",
        "\n",
        "    # Choose optimizer\n",
        "    if optimizer_type == 'adam':\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_type == 'sgd':\n",
        "        optimizer = SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True) # SGD with Nesterov momentum\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer_type. Choose 'adam' or 'sgd'.\")\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# --- 3. Hyper-parameter Optimization / Experimentation ---\n",
        "print(\"\\n--- Starting Hyper-parameter Optimization Experiments ---\")\n",
        "\n",
        "experiments = {\n",
        "    \"Simple CNN - Adam (LR=0.001)\": {\n",
        "        \"architecture_type\": \"simple_cnn\",\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"optimizer_type\": \"adam\",\n",
        "        \"epochs\": 20,\n",
        "        \"batch_size\": 64\n",
        "    },\n",
        "    \"Deeper CNN - Adam (LR=0.001)\": {\n",
        "        \"architecture_type\": \"deeper_cnn\",\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"optimizer_type\": \"adam\",\n",
        "        \"epochs\": 20,\n",
        "        \"batch_size\": 64\n",
        "    },\n",
        "    \"Simple CNN - SGD (LR=0.01)\": {\n",
        "        \"architecture_type\": \"simple_cnn\",\n",
        "        \"learning_rate\": 0.01,\n",
        "        \"optimizer_type\": \"sgd\",\n",
        "        \"epochs\": 20,\n",
        "        \"batch_size\": 64\n",
        "    }\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for exp_name, params in experiments.items():\n",
        "    print(f\"\\n--- Running Experiment: {exp_name} ---\")\n",
        "    model = create_cnn_model(\n",
        "        input_shape=X_train.shape[1:],\n",
        "        num_classes=num_classes,\n",
        "        architecture_type=params[\"architecture_type\"],\n",
        "        learning_rate=params[\"learning_rate\"],\n",
        "        optimizer_type=params[\"optimizer_type\"]\n",
        "    )\n",
        "    model.summary()\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=params[\"epochs\"],\n",
        "        batch_size=params[\"batch_size\"],\n",
        "        validation_split=0.1, # Use a validation split from training data\n",
        "        verbose=1 # Show training progress\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the test data\n",
        "    print(f\"\\n--- Evaluating {exp_name} on Test Set ---\")\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Test Loss: {loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Generate classification report and confusion matrix\n",
        "    y_pred_proba = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Classification Report\n",
        "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    results[exp_name] = {\n",
        "        \"model\": model,\n",
        "        \"history\": history,\n",
        "        \"test_loss\": loss,\n",
        "        \"test_accuracy\": accuracy,\n",
        "        \"classification_report\": report,\n",
        "        \"confusion_matrix\": cm\n",
        "    }\n",
        "\n",
        "# --- 4. Visualization of Results ---\n",
        "print(\"\\n--- Generating Visualizations ---\")\n",
        "\n",
        "for exp_name, res in results.items():\n",
        "    history = res[\"history\"]\n",
        "    cm = res[\"confusion_matrix\"]\n",
        "\n",
        "    # Plot Training & Validation Accuracy and Loss\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'{exp_name} - Accuracy over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{exp_name} - Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(f'{exp_name} - Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- All Experiments Completed ---\")\n",
        "print(\"\\n--- Guidance for Your Report ---\")\n",
        "print(\"This script provides the practical results for Assignment 2. You can use these results to fill out your report sections:\")\n",
        "print(\"1.  **Introduction:** Briefly introduce the problem (CIFAR-10 multi-class classification) and your approach.\")\n",
        "print(\"2.  **Background:** Research and summarize related work on CNNs for image classification, hyper-parameter optimization techniques, and relevant state-of-the-art methods (e.g., deeper architectures, regularization techniques like Batch Normalization and Dropout).\")\n",
        "print(\"3.  **Methodology & Experiments:** Detail your experimental setup. Describe the CIFAR-10 dataset, the preprocessing steps, the CNN architectures you designed ('simple_cnn', 'deeper_cnn'), and the hyper-parameters you varied (learning rate, optimizer, epochs, batch size). Explain your rationale for choosing these variations.\")\n",
        "print(\"4.  **Results:** Present the numerical results (test loss, test accuracy, precision, recall, f1-score from classification report) in tables. Include the generated accuracy/loss plots and confusion matrices as figures. This section should be purely factual presentation of your findings.\")\n",
        "print(\"5.  **Discussion & Conclusion:** Analyze your results. Compare the performance of different architectures and hyper-parameter choices. Discuss which model performed best and why, referencing the metrics and plots. Identify any interesting or counter-intuitive results. Conclude by summarizing your findings and suggesting future work (e.g., trying more advanced architectures like ResNet, using data augmentation, more sophisticated hyper-parameter tuning methods like GridSearchCV or RandomizedSearchCV, or exploring different optimizers/learning rate schedules).\")\n",
        "print(\"6.  **Reflection on Ethical use of AI:** Consider the ethical implications of image classification models, especially in real-world applications. Discuss potential benefits (e.g., automated surveillance for safety, medical image analysis) and potential downsides/abuse (e.g., privacy concerns, bias in facial recognition, misuse in surveillance). Connect this to the CIFAR-10 context (e.g., misclassification of objects in autonomous vehicles).\")\n",
        "print(\"Remember to adhere to the word limit and submission guidelines for your PDF report.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "dnDLVntP5gT3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}